{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataset_API_intro.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DerekChuang/tensorflow_practice/blob/master/Dataset_API_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "1ZpVInAClwdq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![替代文字](https://pic2.zhimg.com/80/v2-f9f42cc5c00573f7baaa815795f1ce45_hd.jpg\n",
        ")"
      ]
    },
    {
      "metadata": {
        "id": "SVjpC6wPmG2H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Dataset可以看作是相同类型“元素”的有序列表\n",
        "\n",
        "先以最简单的，Dataset的每一个元素是一个数字为例："
      ]
    },
    {
      "metadata": {
        "id": "5Y1_axkcmF5q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(np.array([1.0, 2.0, 3.0, 4.0, 5.0]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IPjZuwgzmd-7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "如何将这个dataset中的元素取出呢？方法是从Dataset中示例化一个Iterator，然后对Iterator进行迭代。"
      ]
    },
    {
      "metadata": {
        "id": "TP2O6lV9mGAc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "f66463d3-b77f-4611-893d-9e627a1e5cbb"
      },
      "cell_type": "code",
      "source": [
        "#在非Eager模式下，读取上述dataset中元素的方法为：\n",
        "\n",
        "iterator = dataset.make_one_shot_iterator()\n",
        "one_element = iterator.get_next()\n",
        "with tf.Session() as sess:\n",
        "    for i in range(5):\n",
        "        print(sess.run(one_element))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "2.0\n",
            "3.0\n",
            "4.0\n",
            "5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s7RYQdbzLU3u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "7Qz-EZjFm6rL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "语句iterator = dataset.make_one_shot_iterator()从dataset中实例化了一个Iterator，这个Iterator是一个“one shot iterator”，即只能从头到尾读取一次。one_element = iterator.get_next()表示从iterator里取出一个元素。由于这是非Eager模式，所以one_element只是一个Tensor，并不是一个实际的值。调用sess.run(one_element)后，才能真正地取出一个值。"
      ]
    },
    {
      "metadata": {
        "id": "viyxPeManBTh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "如果一个dataset中元素被读取完了，再尝试sess.run(one_element)的话，就会抛出tf.errors.OutOfRangeError异常，这个行为与使用队列方式读取数据的行为是一致的。在实际程序中，可以在外界捕捉这个异常以判断数据是否读取完，请参考下面的代码："
      ]
    },
    {
      "metadata": {
        "id": "JFXanIbgl4By",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "9a75b4e2-4051-44ad-c747-1d506aae6deb"
      },
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(np.array([1.0, 2.0, 3.0, 4.0, 5.0]))\n",
        "iterator = dataset.make_one_shot_iterator()\n",
        "one_element = iterator.get_next()\n",
        "with tf.Session() as sess:\n",
        "    try:\n",
        "        while True:\n",
        "            print(sess.run(one_element))\n",
        "    except tf.errors.OutOfRangeError:\n",
        "        print(\"end!\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "2.0\n",
            "3.0\n",
            "4.0\n",
            "5.0\n",
            "end!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-k4NSfTMnL2C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "在Eager模式中，创建Iterator的方式有所不同。是通过tfe.Iterator(dataset)的形式直接创建Iterator并迭代。迭代时可以直接取出值，不需要使用sess.run()：\n"
      ]
    },
    {
      "metadata": {
        "id": "JwDd5MndpUbl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 从内存中创建更复杂的Dataset\n"
      ]
    },
    {
      "metadata": {
        "id": "JztIJePvpiCW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "其实，tf.data.Dataset.from_tensor_slices的功能不止如此，它的真正作用是切分传入Tensor的第一个维度，生成相应的dataset。"
      ]
    },
    {
      "metadata": {
        "id": "KTx0fHG2nNZr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(np.random.uniform(size=(5, 2)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RKUkXm_zpqxl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "传入的数值是一个矩阵，它的形状为(5, 2)，tf.data.Dataset.from_tensor_slices就会切分它形状上的第一个维度，最后生成的dataset中一个含有5个元素，每个元素的形状是(2, )，即每个元素是矩阵的一row\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "bhhTFanup9uC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "我们可能还希望Dataset中的每个元素具有更复杂的形式，如每个元素是一个Python中的元组，或是Python中的词典。例如，在图像识别问题中，一个元素可以是{\"image\": image_tensor, \"label\": label_tensor}的形式，这样处理起来更方便。"
      ]
    },
    {
      "metadata": {
        "id": "e9uYMDjOpvVB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    {\n",
        "        \"a\": np.array([1.0, 2.0, 3.0, 4.0, 5.0]),                                       \n",
        "        \"b\": np.random.uniform(size=(5, 2))\n",
        "    }\n",
        ")\n",
        "\n",
        "#最终dataset中的一个元素就是类似于{\"a\": 1.0, \"b\": [0.9, 0.1]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d42pVf4nqUzQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "利用tf.data.Dataset.from_tensor_slices创建每个元素是一个tuple的dataset也是可以的：\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "SU61nMRPqJr6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "  (np.array([1.0, 2.0, 3.0, 4.0, 5.0]), np.random.uniform(size=(5, 2)))\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pfCnD3qmqchh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 对Dataset中的元素做变换：Transformation"
      ]
    },
    {
      "metadata": {
        "id": "PH5w5C7Sqg-K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Dataset支持一类特殊的操作：Transformation。一个Dataset通过Transformation变成一个新的Dataset。通常我们可以通过Transformation完成数据变换，打乱，组成batch，生成epoch等一系列操作。\n",
        "\n",
        "常用的Transformation有：\n",
        "\n",
        "map\n",
        "batch\n",
        "shuffle\n",
        "repeat\n"
      ]
    },
    {
      "metadata": {
        "id": "9FzSNotwqqdK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# （1）map\n",
        "\n",
        "map接收一个函数，Dataset中的每个元素都会被当作这个函数的输入，并将函数返回值作为新的Dataset，如我们可以对dataset中每个元素的值加1："
      ]
    },
    {
      "metadata": {
        "id": "vdYC0VxbqbSZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(np.array([1.0, 2.0, 3.0, 4.0, 5.0]))\n",
        "dataset = dataset.map(lambda x: x + 1) # 2.0, 3.0, 4.0, 5.0, 6.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xMeZIQZZq6KP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# （2）batch\n",
        "\n",
        "batch就是将多个元素组合成batch，如下面的程序将dataset中的每个元素组成了大小为32的batch："
      ]
    },
    {
      "metadata": {
        "id": "yWRv_ZLOqaaL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = dataset.batch(32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TEKcaEX4rD8y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# （3）shuffle\n",
        "\n",
        "shuffle的功能为打乱dataset中的元素，它有一个参数buffersize，表示打乱时使用的buffer的大小："
      ]
    },
    {
      "metadata": {
        "id": "FcJaLZpJrC67",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle(buffer_size=10000)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VrouungErTgL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# （4）repeat\n",
        "\n",
        "repeat的功能就是将整个序列重复多次，主要用来处理机器学习中的epoch，假设原先的数据是一个epoch，使用repeat(5)就可以将之变成5个epoch："
      ]
    },
    {
      "metadata": {
        "id": "8blgcI6RrSxE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = dataset.repeat(5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3h8yEC5drfXp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "如果直接调用repeat()的话，生成的序列就会无限重复下去，没有结束，因此也不会抛出tf.errors.OutOfRangeError异常："
      ]
    },
    {
      "metadata": {
        "id": "a5fc3icTrP77",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = dataset.repeat()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jPMJpW18rm-g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 例子：读入磁盘图片与对应label\n",
        "\n",
        "讲到这里，我们可以来考虑一个简单，但同时也非常常用的例子：读入磁盘中的图片和图片相应的label，并将其打乱，组成batch_size=32的训练样本。在训练时重复10个epoch。"
      ]
    },
    {
      "metadata": {
        "id": "pxcaUsCdsHX3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "4c82573f-c351-4478-f6fb-d91d05d53c08"
      },
      "cell_type": "code",
      "source": [
        "#掛載 google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TRCfbwZjsLeL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "outputId": "f507fc2b-879d-42d1-e124-9de5fce75713"
      },
      "cell_type": "code",
      "source": [
        "!ls '/content/drive/My Drive/太平山照片'\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R0019664.JPG  R0019702.JPG  R0019740.JPG  R0019779.JPG\tR0019822.JPG\n",
            "R0019665.JPG  R0019703.JPG  R0019741.JPG  R0019780.JPG\tR0019823.JPG\n",
            "R0019666.JPG  R0019704.JPG  R0019742.JPG  R0019784.JPG\tR0019824.JPG\n",
            "R0019667.JPG  R0019705.JPG  R0019743.JPG  R0019785.JPG\tR0019825.JPG\n",
            "R0019668.JPG  R0019706.JPG  R0019744.JPG  R0019786.JPG\tR0019826.JPG\n",
            "R0019669.JPG  R0019707.JPG  R0019745.JPG  R0019787.JPG\tR0019827.JPG\n",
            "R0019670.JPG  R0019708.JPG  R0019746.JPG  R0019788.JPG\tR0019828.JPG\n",
            "R0019671.JPG  R0019709.JPG  R0019747.JPG  R0019789.JPG\tR0019829.JPG\n",
            "R0019672.JPG  R0019710.JPG  R0019748.JPG  R0019790.JPG\tR0019830.JPG\n",
            "R0019673.JPG  R0019711.JPG  R0019749.JPG  R0019791.JPG\tR0019831.JPG\n",
            "R0019674.JPG  R0019712.JPG  R0019750.JPG  R0019792.JPG\tR0019832.JPG\n",
            "R0019675.JPG  R0019713.JPG  R0019751.JPG  R0019793.JPG\tR0019833.JPG\n",
            "R0019676.JPG  R0019714.JPG  R0019752.JPG  R0019794.JPG\tR0019834.JPG\n",
            "R0019677.JPG  R0019715.JPG  R0019753.JPG  R0019795.JPG\tR0019835.JPG\n",
            "R0019678.JPG  R0019716.JPG  R0019754.JPG  R0019796.JPG\tR0019836.JPG\n",
            "R0019679.JPG  R0019717.JPG  R0019755.JPG  R0019797.JPG\tR0019837.JPG\n",
            "R0019680.JPG  R0019718.JPG  R0019756.JPG  R0019798.JPG\tR0019838.JPG\n",
            "R0019681.JPG  R0019719.JPG  R0019758.JPG  R0019800.JPG\tR0019839.JPG\n",
            "R0019682.JPG  R0019720.JPG  R0019759.JPG  R0019801.JPG\tR0019840.JPG\n",
            "R0019683.JPG  R0019721.JPG  R0019760.JPG  R0019802.JPG\tR0019842.JPG\n",
            "R0019684.JPG  R0019722.JPG  R0019761.JPG  R0019803.JPG\tR0019843.JPG\n",
            "R0019685.JPG  R0019723.JPG  R0019762.JPG  R0019804.JPG\tR0019844.JPG\n",
            "R0019686.JPG  R0019724.JPG  R0019763.JPG  R0019805.JPG\tR0019845.JPG\n",
            "R0019687.JPG  R0019725.JPG  R0019764.JPG  R0019806.JPG\tR0019846.JPG\n",
            "R0019688.JPG  R0019726.JPG  R0019765.JPG  R0019807.JPG\tR0019847.JPG\n",
            "R0019689.JPG  R0019727.JPG  R0019766.JPG  R0019808.JPG\tR0019848.JPG\n",
            "R0019690.JPG  R0019728.JPG  R0019767.JPG  R0019809.JPG\tR0019849.JPG\n",
            "R0019691.JPG  R0019729.JPG  R0019768.JPG  R0019810.JPG\tR0019850.JPG\n",
            "R0019692.JPG  R0019730.JPG  R0019769.JPG  R0019811.JPG\tR0019851.JPG\n",
            "R0019693.JPG  R0019731.JPG  R0019770.JPG  R0019813.JPG\tR0019852.JPG\n",
            "R0019694.JPG  R0019732.JPG  R0019771.JPG  R0019814.JPG\tR0019853.JPG\n",
            "R0019695.JPG  R0019733.JPG  R0019772.JPG  R0019815.JPG\tR0019854.JPG\n",
            "R0019696.JPG  R0019734.JPG  R0019773.JPG  R0019816.JPG\tR0019855.JPG\n",
            "R0019697.JPG  R0019735.JPG  R0019774.JPG  R0019817.JPG\tR0019856.JPG\n",
            "R0019698.JPG  R0019736.JPG  R0019775.JPG  R0019818.JPG\tR0019857.JPG\n",
            "R0019699.JPG  R0019737.JPG  R0019776.JPG  R0019819.JPG\tR0019858.JPG\n",
            "R0019700.JPG  R0019738.JPG  R0019777.JPG  R0019820.JPG\tR0019859.JPG\n",
            "R0019701.JPG  R0019739.JPG  R0019778.JPG  R0019821.JPG\tR0019860.JPG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ATBZW62BsLav",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9SyEAxOmsLXM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd742737-8166-439d-f6cb-fcbce3669f18"
      },
      "cell_type": "code",
      "source": [
        "! ls '/content/drive' "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'My Drive'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TY9b-LFTrhuU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1425
        },
        "outputId": "6cb9e0c8-2172-40ca-e6b2-a6a190bba202"
      },
      "cell_type": "code",
      "source": [
        "# 函数的功能时将filename对应的图片文件读进来，并缩放到统一的大小\n",
        "def _parse_function(filename, label):\n",
        "  image_string = tf.read_file(filename)\n",
        "  image_decoded = tf.image.decode_image(image_string)\n",
        "  image_resized = tf.image.resize_images(image_decoded, [28, 28])\n",
        "  return image_resized, label\n",
        "\n",
        "# 图片文件的列表\n",
        "filenames = tf.constant([\"/content/drive/My Drive/tp_photo/R0019664.jpg\"])\n",
        "# label[i]就是图片filenames[i]的label\n",
        "labels = tf.constant([0])\n",
        "\n",
        "# 此时dataset中的一个元素是(filename, label)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
        "\n",
        "# 此时dataset中的一个元素是(image_resized, label)\n",
        "dataset = dataset.map(_parse_function)\n",
        "\n",
        "# 此时dataset中的一个元素是(image_resized_batch, label_batch)\n",
        "dataset = dataset.shuffle(buffersize=1000).batch(32).repeat(10)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-b1bc26806fb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# 此时dataset中的一个元素是(image_resized, label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_parse_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# 此时dataset中的一个元素是(image_resized_batch, label_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \"\"\"\n\u001b[1;32m   1037\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mParallelMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism)\u001b[0m\n\u001b[1;32m   2609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2610\u001b[0m     wrapped_func = StructuredFunctionWrapper(\n\u001b[0;32m-> 2611\u001b[0;31m         map_func, \"Dataset.map()\", input_dataset)\n\u001b[0m\u001b[1;32m   2612\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2613\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, add_to_graph, experimental_nested_dataset_support)\u001b[0m\n\u001b[1;32m   1858\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_data_structured_function_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1859\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1861\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m       \u001b[0;31m# Use the private method that will execute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36madd_to_graph\u001b[0;34m(self, g)\u001b[0m\n\u001b[1;32m    477\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;34m\"\"\"Adds this function into the graph g.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_definition_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Adds this function into 'g'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36m_create_definition_if_needed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;34m\"\"\"Creates the function definition if it's not created yet.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_definition_if_needed_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_definition_if_needed_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36m_create_definition_if_needed_impl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    342\u001b[0m     temp_graph = func_graph_from_py_func(\n\u001b[1;32m    343\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arg_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arg_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         self._capture_by_value, self._caller_device)\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extra_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/function.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(func, arg_names, arg_types, name, capture_by_value, device, colocation_stack, container, collections_ref, arg_shapes)\u001b[0m\n\u001b[1;32m    862\u001b[0m     \u001b[0;31m# Call func and gather the output tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m     \u001b[0;31m# There is no way of distinguishing between a function not returning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mtf_data_structured_function_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1792\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1794\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1795\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1796\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-b1bc26806fb5>\u001b[0m in \u001b[0;36m_parse_function\u001b[0;34m(filename, label)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mimage_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mimage_decoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mimage_resized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_decoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mimage_resized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/image_ops_impl.py\u001b[0m in \u001b[0;36mresize_images\u001b[0;34m(images, size, method, align_corners, preserve_aspect_ratio)\u001b[0m\n\u001b[1;32m    996\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\'images\\' contains no shape.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m     \u001b[0;31m# TODO(shlens): Migrate this functionality to the underlying Op's.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m     \u001b[0mis_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 'images' contains no shape."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "CRnGrLjFt5oZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "EqZmJpoSs8yA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eIVCW8vgLXcm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://www.jianshu.com/p/8c168fc0e01e"
      ]
    },
    {
      "metadata": {
        "id": "fr6uoAaNLYYo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}